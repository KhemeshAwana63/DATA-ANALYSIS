{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------CHAPTER 6-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas.read_csv Arguments: The Wild World of CSV Parsing\n",
    "- `path`: The VIP pass to your data party—could be a filesystem location (like \"C:/data.csv\"), a URL (for those fancy online files), or a file-like object. Tell pandas where the treasure is buried!\n",
    "- `sep or delimiter`: The bouncer that splits your row into fields. Give it a character (`,` for commas), or go wild with a regex (like `\\s+` for sneaky spaces). No sep, no entry!\n",
    "- `header`: Which row gets to wear the crown as column names? Defaults to 0 (first row), but set it to `None` if your file’s too cool for headers. No header, no problem!\n",
    "- `index_col`: Pick your row label MVPs—column numbers or names to use as the index. Want a fancy hierarchical index? Toss in a list like `[\"key1\", \"key2\"]` and watch the magic happen.\n",
    "- `names`: Your chance to play god and name those columns yourself. Pass a list like `[\"a\", \"b\", \"c\"]`—because who needs default 0, 1, 2 nonsense?\n",
    "- `skiprows`: Rows to kick to the curb—either a number (skip the first N rows) or a list of row numbers (starting at 0). Perfect for dodging pesky comments or that awkward intro text.\n",
    "- `na_values`: The bouncer’s blacklist—values like `\"NA\"`, `\"NULL\"`, or whatever you hate that get swapped for `NaN`. Add your own, but they join the default crew unless you say `keep_default_na=False`.\n",
    "- `keep_default_na`: To roll with pandas’ default `NaN` squad (like `\"NA\"`, `\"NULL\"`) or not? `True` by default, but flip it to `False` if you’re a rebel who hates surprises.\n",
    "- `comment`: The “shush” button—characters (like `#`) that mark the start of comments to ignore. Keeps your data free of chit-chat at the end of lines.\n",
    "- `parse_dates`: Date whisperer mode—set to `False` by default (no parsing). Flip to `True` to sniff out dates in all columns, or give a list of columns (`[0, 1]` or `[\"date\"]`) to focus on. Bonus: combine columns with a tuple like `[\"date\", \"time\"]`!\n",
    "- `keep_date_col`: After parsing dates from multiple columns, keep the originals around? `False` by default—pandas is tidy like that—but set to `True` if you’re sentimental.\n",
    "- `converters`: Your personal data stylist—a dictionary like `{\"foo\": lambda x: x.upper()}` to transform column values. Make “foo” shout in caps if you feel like it!\n",
    "- `dayfirst`: For those ambiguous dates (7/6/2012)—treat as international style (June 7, 2012) with `True`. `False` by default, because pandas assumes you’re an American date snob.\n",
    "- `date_parser`: Bring your own date-parsing chef—pass a function to cook those dates just right. Default is pandas’ built-in guesswork.\n",
    "- `nrows`: How much of the file to nibble? Set a number (e.g., 10) to read only the first N rows (excluding header). Perfect for a quick taste test!\n",
    "- `iterator`: Turn your file into a lazy reader—returns a `TextFileReader` object to process it bit by bit. Pairs nicely with a `with` statement for that classy vibe.\n",
    "- `chunksize`: For big file feasts, set the chunk size (e.g., 1000 rows) and munch through it piece by piece. Requires `iterator=True` to activate!\n",
    "- `skip_footer`: Chop off the end—number of lines to ignore at the file’s bottom. Because who cares about the fine print?\n",
    "- `verbose`: Chatty mode—set to `True` to get the gossip on parsing times and memory use. Great for nerds who love the behind-the-scenes drama.\n",
    "- `encoding`: The secret handshake for text—defaults to `\"utf-8\"`, but tweak it (e.g., `\"latin1\"`) if your file’s speaking a different language.\n",
    "- `squeeze`: If your data’s a one-column wonder, set to `True` to squeeze it into a `Series` instead of a DataFrame. Less clutter, more swagger!\n",
    "- `thousands`: The bling separator for big numbers—like `\",\"` in \"1,000\" or `\".\"` in \"1.000\". Default is `None`, so pandas won’t guess your style.\n",
    "- `decimal`: The decimal diva—usually `\".\"` (like 3.14), but switch to `\",\"` (like 3,14) if your file’s got European flair. Default is `\".\"`.\n",
    "- `engine`: The horsepower under the hood—`\"c\"` (default, fast), `\"python\"` (slower but feature-rich), or `\"pyarrow\"` (new kid, blazing fast). Pick your ride wisely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.csv\") #1st row is header\n",
    "#IF NO HEADER\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex2.csv\", header=None) #default header is 0\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex2.csv\", names=['a', 'b', 'c', 'd', 'message']) #specify header\n",
    "#INDEXING ROWS\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.csv\", index_col='message') #set message as index\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/csv_mindex.csv\", index_col=[\"key1\" ,\"key2\"]) #set key1 and key2 as index\n",
    "#HANDLING VARIABLE WHITESPACE\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex3.txt\", sep=r'\\s+') #use regex for whitespace and and first column becomes index because there are one less column header\n",
    "#SKIP ROWS\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex4.csv\", skiprows=[0, 2, 3]) #skip rows 0, 2, 3\n",
    "#HANDLING MISSING VALUES\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\", na_values=['NULL']) #replace NULL with NaN\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\", na_values={'message': ['foo', 'NA'], 'something': ['two']}) #replace foo and NA in message and two in something with NaN\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\", keep_default_na=False) #do not replace default NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READING THE TEXT FILE IN PIECES**\n",
    "- Chunksize -> it is a textFileReader object that is a iterable ..it gives a specified amount of rows in each iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex6.csv\")\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex6.csv\", nrows=5) #read only 5 rows\n",
    "chunker = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex6.csv\", chunksize=1000) #read in chunks\n",
    "type(chunker)\n",
    "\n",
    "tot = pd.Series([],dtype='float64')\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece[\"key\"].value_counts(),fill_value=0)\n",
    "tot.sort_values(ascending=False) #sort values in descending order\n",
    "\n",
    "chunker.get_chunk(5) #get 5 rows from chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WRITING DATA TO TEXT FORMAT ->we are using sys.stdout -> so that it only prints the data on console and not make a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\")\n",
    "data.to_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/out.csv\") #dumps the data of ex5 to a new file out.csv\n",
    "#CUSTOMIZATIONS THAT WE CAN DO\n",
    "import sys\n",
    "data.to_csv(sys.stdout , sep=\"|\") #separate by |\n",
    "data.to_csv(sys.stdout , na_rep=\"NULL\") #replace NaN with NULL\n",
    "data.to_csv(sys.stdout , index=False , header=False) #do not write index and header\n",
    "data.to_csv(sys.stdout , index=False , columns=[\"a\",\"b\",\"c\"]) #write only columns a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORKING WITH OTHER DELEMITED FORMATS**\n",
    "- if you don't wanna  use pandas, you can use csv module\n",
    "- csv.reader -> it creates lines in lists\n",
    "- delimiter: Field alag karne ka character (default ,).\n",
    "- lineterminator: Line end (default \\r\\n).\n",
    "- quotechar: Quotes ke liye (default \").\n",
    "- quoting: Kitna quote karna hai (MINIMAL, ALL, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex7.csv\")\n",
    "reader = csv.reader(f)\n",
    "for line in reader:\n",
    "    print(line)            #read csv file using csv module and return list of lines\n",
    "\n",
    "with open(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex7.csv\") as f:\n",
    "    lines = list(csv.reader(f))\n",
    "header , *values = lines\n",
    "data_dict = {h:v for h , v in zip(header , zip(*values))}\n",
    "\n",
    "\"\"\"dialect drama\"\"\"\n",
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "reader = csv.reader(f , dialect=my_dialect) #read csv file with custom dialect\n",
    "reader = csv.reader(f , delimiter='|') #shortcut -> read csv file with custom delimiter\n",
    "\n",
    "\"\"\"manual writing\"\"\"\n",
    "writer = csv.writer(f , dialect=my_dialect)\n",
    "writer.writerow(('one', 'two', 'three'))\n",
    "writer.writerow(('1', '2', '3'))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON**\n",
    "- null of json is None in python\n",
    "- JSON Types: Dictionary (object), list (array), string, number, boolean, null.\n",
    "- Reading: json.loads (string to Python), pd.read_json (file to DataFrame).\n",
    "- Writing: json.dumps (Python to string), to_json (DataFrame to JSON).\n",
    "- Customization: orient=\"records\" or any specific column you can choose.\n",
    "- More flexible then csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERTING FROM JSON TO PYTHON AND PYTHON TO JSON\n",
    "import json\n",
    "obj = \"\"\"obj ={\"name\": \"Wes\",\n",
    " \"cities_lived\": [\"Akron\", \"Nashville\", \"New York\", \"San Francisco\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 34, \"hobbies\": [\"guitars\", \"soccer\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 42, \"hobbies\": [\"diving\", \"art\"]}]\n",
    "} \"\"\"\n",
    "\n",
    "result = json.loads(obj) #convert json to python\n",
    "asjson = json.dumps(result) #convert python to json\n",
    "\n",
    "#NOW WE ARE GONNA SEE HOW TO CONVERT JSON TO PANDAS DATAFRAME\n",
    "data = pd.read_json(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/example.json\") #direct method\n",
    "siblings = pd.DataFrame(result[\"siblings\"],columns=[\"name\",\"age\"]) #manual/specific method\n",
    "\n",
    "#CONVERTING DATAFRAME TO JSON\n",
    "data.to_json(sys.stdout)\n",
    "data.to_json(sys.stdout , orient='records') #convert to json with records orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XML AND HTML -> WEB SCRAPPING**\n",
    "- Libraries: lxml (fast), BeautifulSoup aur html5lib (flexible) install them for read_html\n",
    "- read_html: It returns a list and converts table with <table> tag in HTML to dataframes\n",
    "- Analysis: Once converted , you can do the analysis\n",
    "- It is used to deal with web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/fdic_failed_bank_list.html\")\n",
    "len(tables) #number of tables\n",
    "failures = tables[0] #get the first table\n",
    "failures.head() #get first 5 rows\n",
    "\n",
    "close_timestamps = pd.to_datetime(failures[\"Closing Date\"])\n",
    "close_timestamps.dt.year.value_counts() #count the number of failures per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING XML WITH lxml.objectify \n",
    "**Summary of the Manual Method Pathway**\n",
    "- We took an XML file with MTA performance data and processed it step-by-step. First, we opened the file and read its structure. Then, we grabbed the main section (root) containing all the records. Next, we looped through each record, picked out the useful info (skipping unwanted parts), and stored it in small data packets. Finally, we turned those packets into a neat table to see the data clearly.\n",
    "--------\n",
    "**functions used**\n",
    "- objectify.parse() - Reads the XML file and turns it into a format we can work with.\n",
    "- parsed.getroot() - Gets the top part (root) of the XML file where all the records start.\n",
    "- elt.getchildren() - Lists all the small pieces (tags) inside each record.\n",
    "- child.pyval - Extracts the actual value (like \"2008\" or \"Metro-North Railroad\") from each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import objectify\n",
    "path = \"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/datasets/mta_perf/Performance_MNR.xml\"\n",
    "with open(path) as f:\n",
    "    parsed = objectify.parse(f)\n",
    "\n",
    "root = parsed.getroot()\n",
    "\n",
    "data =[]\n",
    "skip_fields = [\"PARENT_SEQ\", \"INDICATOR_SEQ\", \"DESIRED_CHANGE\", \"DECIMAL_PLACES\"]\n",
    "for elt in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)\n",
    "\n",
    "perf = pd.DataFrame(data)\n",
    "perf.head()\n",
    "\n",
    "\"\"\"MENTOS JINDAGI\"\"\"\n",
    "perf2 = pd.read_xml(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BINARY DATA FORMAT**\n",
    "- `PICKLE` -> note :- Use this for on short term data storage because this data format is very unstable and future libraries may not be able to support this file format \n",
    "    - Threre are more binary data format files like apeche parquet , ORC , HDF5\n",
    "- `EXCEL` -> When the data has many sheet then the ExcelFile is prefered to read the file rather then read_excel\n",
    "    - it requires libraries -> openpyxl , xlrd\n",
    "- `HDF5` - hiearchal data format used for large datasets\n",
    "    - requires libraries -> pytables under the hood\n",
    "    - to handle HDF5 work we have HDFStore class in pandas\n",
    "    - to save data in HDF5 we have two options (fixed and tables -> fixed is by default)\n",
    "    - fixed -> fast and can't do queries\n",
    "    - tables -> slow but can do queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "frame = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.csv\")\n",
    "frame.to_pickle(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/frame_pickle\")\n",
    "\n",
    "pd.read_pickle(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/frame_pickle\") #newer approach\n",
    "with open(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/frame_pickle\",\"rb\") as f: #older approach\n",
    "    file = pickle.load(f)\n",
    "\n",
    "fec = pd.read_parquet(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/datasets/fec/fec.parquet\") #another binary data format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLDER APPROACH\n",
    "xlsx = pd.ExcelFile(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.xlsx\") #it returns a object that has full blueprint of the file\n",
    "xlsx.sheet_names #\"Sheet1\" -> has only one sheet in it\n",
    "xlsx.parse(sheet_name=\"Sheet1\" , index_col=0)  #Check out why i used index_col = 0\n",
    "\n",
    "#NEWER APPROACH\n",
    "frame = pd.read_excel(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.xlsx\" ,sheet_name=\"Sheet1\",index_col=0)\n",
    "\n",
    "#HERE IS THE EXAMPLE OF WRITING THE DATA TO EXCEL FILE (OLDER AND NEWER VERSIONS)\n",
    "writer = pd.ExcelWriter(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.xlsx\")\n",
    "frame.to_excel(writer , \"Sheet1\")\n",
    "writer.save()\n",
    "frame.to_excel(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"MANUAL METHOD\"\"\"\n",
    "frame = pd.DataFrame({\"a\": np.random.standard_normal(100)})  # 100 random numbers ka DataFrame\n",
    "store = pd.HDFStore(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/mydata.h5\")  # HDF5 file kholi\n",
    "\n",
    "store[\"obj1\"] = frame  # Pura DataFrame save kiya\n",
    "store[\"obj1_col\"] = frame[\"a\"]  # Sirf \"a\" column save kiya\n",
    "store  # File ka status check\n",
    "store[\"obj1\"]  # Pura DataFrame nikala\n",
    "store[\"obj1_col\"]  # Sirf \"a\" column nikala\n",
    "store.put(\"obj2\", frame, format=\"table\")  # \"table\" format mein save kiya querying ke liye\n",
    "store.select(\"obj2\", where=[\"index >= 10 and index <= 15\"])  # Index 10-15 tak ka data query kiya\n",
    "store.close()  # File band ki\n",
    "\n",
    "\"\"\"SHORTCUT METHOD\"\"\"\n",
    "frame.to_hdf(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/mydata.h5\", \"obj3\", format=\"table\")  # Seedha DataFrame ko HDF5 mein save\n",
    "pd.read_hdf(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/mydata.h5\", \"obj3\", where=[\"index < 5\"])  # Index 0-4 tak ka data padha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API'S**\n",
    "- Web API is a data , given by the website to us and it can be of any data format like json etc\n",
    "- request is a module that works like a delivery boy who gets the data and gives us <br>\n",
    "it sends a HTTP GET request to the \n",
    "- Always use raise_for_error to avoid conflict\n",
    "- The data here is Real Time Data so it might change everytime you try to get\n",
    "- the response that you get is a list containing all the elments in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status() #do this to check if everything happend succesfully\n",
    "resp #if it returns <Response [200]> then it was succesfull\n",
    "\n",
    "data = resp.json()\n",
    "data[0][\"title\"] #this fetches the data from title \"key\"\"CHAPTER 2.ipynb\"\n",
    "\n",
    "issues = pd.DataFrame(data , columns=[\"number\" , \"title\" , \"labels\" , \"state\"]) #creating dataframe with specified columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATABASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "query = \"\"\"\n",
    "        CREATE TABLE test\n",
    "        (a VARCHAR(20) , b VARCHAR(20) ,\n",
    "        c REAL ,         d INTEGER\n",
    "        );\"\"\"\n",
    "con = sqlite3.connect(\"mydata.sqlite\")  # Connects to SQLite file \"mydata.sqlite\" (creates if not exists)\n",
    "con.execute(query)  # Executes the query to create the table\n",
    "con.commit()  # Saves the table creation to the database\n",
    "\n",
    "data = [(\"Atlanta\", \"Georgia\", 1.25, 6),\n",
    "        (\"Tallahassee\", \"Florida\", 2.6, 3),\n",
    "        (\"Sacramento\", \"California\", 1.7, 5)]\n",
    "\n",
    "stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"  # SQL insert statement with placeholders for values\n",
    "con.executemany(stmt, data)  # Inserts data into table (Note: should be executemany for multiple rows, this will error)\n",
    "con.commit()\n",
    "\n",
    "cursor = con.execute(\"SELECT * FROM test\")  # Runs query to select all data, returns a cursor object\n",
    "cursor.description  # Shows column info (names like 'a', 'b', etc.)\n",
    "rows = cursor.fetchall()  # Fetches all rows from the query as a list of tuples\n",
    "\n",
    "pd.DataFrame(rows, columns=[x[0] for x in cursor.description])  # Converts rows to DataFrame with column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sqla\n",
    "db = sqla.create_engine(\"sqlite:///mydata.sqlite\")\n",
    "pd.read_sql(\"SELECT * FROM test\" , db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
