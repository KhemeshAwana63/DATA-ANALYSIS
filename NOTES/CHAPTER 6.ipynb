{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------CHAPTER 6-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas.read_csv Arguments: The Wild World of CSV Parsing\n",
    "- `path`: The VIP pass to your data party—could be a filesystem location (like \"C:/data.csv\"), a URL (for those fancy online files), or a file-like object. Tell pandas where the treasure is buried!\n",
    "- `sep or delimiter`: The bouncer that splits your row into fields. Give it a character (`,` for commas), or go wild with a regex (like `\\s+` for sneaky spaces). No sep, no entry!\n",
    "- `header`: Which row gets to wear the crown as column names? Defaults to 0 (first row), but set it to `None` if your file’s too cool for headers. No header, no problem!\n",
    "- `index_col`: Pick your row label MVPs—column numbers or names to use as the index. Want a fancy hierarchical index? Toss in a list like `[\"key1\", \"key2\"]` and watch the magic happen.\n",
    "- `names`: Your chance to play god and name those columns yourself. Pass a list like `[\"a\", \"b\", \"c\"]`—because who needs default 0, 1, 2 nonsense?\n",
    "- `skiprows`: Rows to kick to the curb—either a number (skip the first N rows) or a list of row numbers (starting at 0). Perfect for dodging pesky comments or that awkward intro text.\n",
    "- `na_values`: The bouncer’s blacklist—values like `\"NA\"`, `\"NULL\"`, or whatever you hate that get swapped for `NaN`. Add your own, but they join the default crew unless you say `keep_default_na=False`.\n",
    "- `keep_default_na`: To roll with pandas’ default `NaN` squad (like `\"NA\"`, `\"NULL\"`) or not? `True` by default, but flip it to `False` if you’re a rebel who hates surprises.\n",
    "- `comment`: The “shush” button—characters (like `#`) that mark the start of comments to ignore. Keeps your data free of chit-chat at the end of lines.\n",
    "- `parse_dates`: Date whisperer mode—set to `False` by default (no parsing). Flip to `True` to sniff out dates in all columns, or give a list of columns (`[0, 1]` or `[\"date\"]`) to focus on. Bonus: combine columns with a tuple like `[\"date\", \"time\"]`!\n",
    "- `keep_date_col`: After parsing dates from multiple columns, keep the originals around? `False` by default—pandas is tidy like that—but set to `True` if you’re sentimental.\n",
    "- `converters`: Your personal data stylist—a dictionary like `{\"foo\": lambda x: x.upper()}` to transform column values. Make “foo” shout in caps if you feel like it!\n",
    "- `dayfirst`: For those ambiguous dates (7/6/2012)—treat as international style (June 7, 2012) with `True`. `False` by default, because pandas assumes you’re an American date snob.\n",
    "- `date_parser`: Bring your own date-parsing chef—pass a function to cook those dates just right. Default is pandas’ built-in guesswork.\n",
    "- `nrows`: How much of the file to nibble? Set a number (e.g., 10) to read only the first N rows (excluding header). Perfect for a quick taste test!\n",
    "- `iterator`: Turn your file into a lazy reader—returns a `TextFileReader` object to process it bit by bit. Pairs nicely with a `with` statement for that classy vibe.\n",
    "- `chunksize`: For big file feasts, set the chunk size (e.g., 1000 rows) and munch through it piece by piece. Requires `iterator=True` to activate!\n",
    "- `skip_footer`: Chop off the end—number of lines to ignore at the file’s bottom. Because who cares about the fine print?\n",
    "- `verbose`: Chatty mode—set to `True` to get the gossip on parsing times and memory use. Great for nerds who love the behind-the-scenes drama.\n",
    "- `encoding`: The secret handshake for text—defaults to `\"utf-8\"`, but tweak it (e.g., `\"latin1\"`) if your file’s speaking a different language.\n",
    "- `squeeze`: If your data’s a one-column wonder, set to `True` to squeeze it into a `Series` instead of a DataFrame. Less clutter, more swagger!\n",
    "- `thousands`: The bling separator for big numbers—like `\",\"` in \"1,000\" or `\".\"` in \"1.000\". Default is `None`, so pandas won’t guess your style.\n",
    "- `decimal`: The decimal diva—usually `\".\"` (like 3.14), but switch to `\",\"` (like 3,14) if your file’s got European flair. Default is `\".\"`.\n",
    "- `engine`: The horsepower under the hood—`\"c\"` (default, fast), `\"python\"` (slower but feature-rich), or `\"pyarrow\"` (new kid, blazing fast). Pick your ride wisely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.csv\") #1st row is header\n",
    "#IF NO HEADER\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex2.csv\", header=None) #default header is 0\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex2.csv\", names=['a', 'b', 'c', 'd', 'message']) #specify header\n",
    "#INDEXING ROWS\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex1.csv\", index_col='message') #set message as index\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/csv_mindex.csv\", index_col=[\"key1\" ,\"key2\"]) #set key1 and key2 as index\n",
    "#HANDLING VARIABLE WHITESPACE\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex3.txt\", sep=r'\\s+') #use regex for whitespace and and first column becomes index because there are one less column header\n",
    "#SKIP ROWS\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex4.csv\", skiprows=[0, 2, 3]) #skip rows 0, 2, 3\n",
    "#HANDLING MISSING VALUES\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\", na_values=['NULL']) #replace NULL with NaN\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\", na_values={'message': ['foo', 'NA'], 'something': ['two']}) #replace foo and NA in message and two in something with NaN\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\", keep_default_na=False) #do not replace default NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READING THE TEXT FILE IN PIECES**\n",
    "- Chunksize -> it is a textFileReader object that is a iterable ..it gives a specified amount of rows in each iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex6.csv\")\n",
    "pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex6.csv\", nrows=5) #read only 5 rows\n",
    "chunker = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex6.csv\", chunksize=1000) #read in chunks\n",
    "type(chunker)\n",
    "\n",
    "tot = pd.Series([],dtype='float64')\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece[\"key\"].value_counts(),fill_value=0)\n",
    "tot.sort_values(ascending=False) #sort values in descending order\n",
    "\n",
    "chunker.get_chunk(5) #get 5 rows from chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WRITING DATA TO TEXT FORMAT ->we are using sys.stdout -> so that it only prints the data on console and not make a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex5.csv\")\n",
    "data.to_csv(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/out.csv\") #dumps the data of ex5 to a new file out.csv\n",
    "#CUSTOMIZATIONS THAT WE CAN DO\n",
    "import sys\n",
    "data.to_csv(sys.stdout , sep=\"|\") #separate by |\n",
    "data.to_csv(sys.stdout , na_rep=\"NULL\") #replace NaN with NULL\n",
    "data.to_csv(sys.stdout , index=False , header=False) #do not write index and header\n",
    "data.to_csv(sys.stdout , index=False , columns=[\"a\",\"b\",\"c\"]) #write only columns a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORKING WITH OTHER DELEMITED FORMATS**\n",
    "- if you don't wanna  use pandas, you can use csv module\n",
    "- csv.reader -> it creates lines in lists\n",
    "- delimiter: Field alag karne ka character (default ,).\n",
    "- lineterminator: Line end (default \\r\\n).\n",
    "- quotechar: Quotes ke liye (default \").\n",
    "- quoting: Kitna quote karna hai (MINIMAL, ALL, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex7.csv\")\n",
    "reader = csv.reader(f)\n",
    "for line in reader:\n",
    "    print(line)            #read csv file using csv module and return list of lines\n",
    "\n",
    "with open(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/ex7.csv\") as f:\n",
    "    lines = list(csv.reader(f))\n",
    "header , *values = lines\n",
    "data_dict = {h:v for h , v in zip(header , zip(*values))}\n",
    "\n",
    "\"\"\"dialect drama\"\"\"\n",
    "class my_dialect(csv.Dialect):\n",
    "    lineterminator = '\\n'\n",
    "    delimiter = ';'\n",
    "    quotechar = '\"'\n",
    "    quoting = csv.QUOTE_MINIMAL\n",
    "reader = csv.reader(f , dialect=my_dialect) #read csv file with custom dialect\n",
    "reader = csv.reader(f , delimiter='|') #shortcut -> read csv file with custom delimiter\n",
    "\n",
    "\"\"\"manual writing\"\"\"\n",
    "writer = csv.writer(f , dialect=my_dialect)\n",
    "writer.writerow(('one', 'two', 'three'))\n",
    "writer.writerow(('1', '2', '3'))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON**\n",
    "- null of json is None in python\n",
    "- JSON Types: Dictionary (object), list (array), string, number, boolean, null.\n",
    "- Reading: json.loads (string to Python), pd.read_json (file to DataFrame).\n",
    "- Writing: json.dumps (Python to string), to_json (DataFrame to JSON).\n",
    "- Customization: orient=\"records\" or any specific column you can choose.\n",
    "- More flexible then csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERTING FROM JSON TO PYTHON AND PYTHON TO JSON\n",
    "import json\n",
    "obj = \"\"\"obj ={\"name\": \"Wes\",\n",
    " \"cities_lived\": [\"Akron\", \"Nashville\", \"New York\", \"San Francisco\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 34, \"hobbies\": [\"guitars\", \"soccer\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 42, \"hobbies\": [\"diving\", \"art\"]}]\n",
    "} \"\"\"\n",
    "\n",
    "result = json.loads(obj) #convert json to python\n",
    "asjson = json.dumps(result) #convert python to json\n",
    "\n",
    "#NOW WE ARE GONNA SEE HOW TO CONVERT JSON TO PANDAS DATAFRAME\n",
    "data = pd.read_json(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/example.json\") #direct method\n",
    "siblings = pd.DataFrame(result[\"siblings\"],columns=[\"name\",\"age\"]) #manual/specific method\n",
    "\n",
    "#CONVERTING DATAFRAME TO JSON\n",
    "data.to_json(sys.stdout)\n",
    "data.to_json(sys.stdout , orient='records') #convert to json with records orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XML AND HTML -> WEB SCRAPPING**\n",
    "- Libraries: lxml (fast), BeautifulSoup aur html5lib (flexible) install them for read_html\n",
    "- read_html: It returns a list and converts table with <table> tag in HTML to dataframes\n",
    "- Analysis: Once converted , you can do the analysis\n",
    "- It is used to deal with web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/examples/fdic_failed_bank_list.html\")\n",
    "len(tables) #number of tables\n",
    "failures = tables[0] #get the first table\n",
    "failures.head() #get first 5 rows\n",
    "\n",
    "close_timestamps = pd.to_datetime(failures[\"Closing Date\"])\n",
    "close_timestamps.dt.year.value_counts() #count the number of failures per year"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
