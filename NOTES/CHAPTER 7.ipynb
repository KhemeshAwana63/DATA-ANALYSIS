{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------ CHAPTER 7 ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDLING MISSING VALUES**\n",
    "- NaN - Not a number and NA - not available\n",
    "- We can filter the missing values using boolean indexing but there are specific methods made for this task\n",
    "    - dropna \n",
    "    - fillna\n",
    "    - isna\n",
    "    - notna\n",
    "- In case of dataframes the whole row or column is being dropped\n",
    "    - how=\"all\" -> drops rows/column which has all NaN\n",
    "    - thresh -> here you specify a perticular number of NaN on the basis of that it selects which row or column to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.Series([1, np.nan, 3.5, np.nan, 7])\n",
    "data.dropna()\n",
    "data[data.notna()]\n",
    "data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],[np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])\n",
    "data.dropna()\n",
    "data.dropna(how=\"all\" , axis=\"columns\") #drops rows/column with all null values\n",
    "data.dropna(axis=\"columns\", thresh= 2) #drops rows/columns with min 2 null values\n",
    "\n",
    "data.fillna(0)\n",
    "data.fillna({1:0.5 , 2:0}) #choosing what to fill in what column using dictionary\n",
    "data.fillna(method=\"ffill\" , limit=2) #forward fill and limit is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDLING DUPLICATES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"], \"k2\": [1,1, 2, 3, 3, 4, 4]})\n",
    "data.duplicated() #gives a boolean object\n",
    "data.drop_duplicates(subset=[\"k1\"],keep=\"last\") #In subset we can choose multiple columns and keep tells which values to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORMING THE DATA USING MAPPING AND FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"food\": [\"bacon\", \"pulled pork\", \"bacon\", \"pastrami\", \"corned beef\", \"bacon\", \"pastrami\", \"honey ham\", \"nova lox\"],\"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
    "meat_to_animal = {\"bacon\": \"pig\",\"pulled pork\": \"pig\",\"pastrami\": \"cow\",\"corned beef\": \"cow\",\"honey ham\": \"pig\",\"nova lox\": \"salmon\"}\n",
    "data[\"animal\"] = data[\"food\"].map(meat_to_animal) #this will map all the animals according to the dictionary\n",
    "\n",
    "def get_animal(x): #this is a function based approach\n",
    "    return meat_to_animal(x)\n",
    "data[\"food\"].map(get_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REPLACING VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "data.replace(-999, np.nan) #replacing value\n",
    "data.replace([-999 , -1000],np.nan) #replacing multiple values\n",
    "data.replace([-999 , -1000],[np.nan , 0]) #choosing different value for different element\n",
    "data.replace({-999:np.nan , -1000:0}) #by using dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RENAMING AXEX INDEXES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.arange(12).reshape((3, 4)), index=[\"Ohio\",\"Colorado\", \"New York\"], columns=[\"one\", \"two\", \"three\", \"four\"])\n",
    "def transform(x):\n",
    "    return x[:4].upper()\n",
    "data.index = data.index.map(transform)  #it will change the index to uppercase for all characteers and this method affect the original dataframe\n",
    "\n",
    "data.rename(index=str.title , columns=str.upper) #This does not change the original dataframe\n",
    "data.rename(index={\"OHIO\":\"INDIANA\"},columns={\"three\":\"pikaboo\"}) #using dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCRETIZATION AND BINNING**\n",
    "- It is like dividing continous data into discrete bins like age into groups\n",
    "- see the output of pd.cut() -> it is good to see that once for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "age_categories = pd.cut(ages , bin , right = False) #it categorize the ages on making intervals using bins and by default the right side of the interval is closed\n",
    "age_categories.value_counts()\n",
    "\n",
    "data = np.random.uniform(size=20)\n",
    "pd.cut(data , 4 , precision=2) #equal length bins\n",
    "\n",
    "data = np.random.standard_normal(1000) #quantile based bins(qcut)\n",
    "quartiles = pd.cut(data , 4 , precision=2)\n",
    "quartiles.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DETECTING AND FILTRING OUTLIERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.random.standard_normal((1000 , 4)))\n",
    "col = data[2]\n",
    "col[col.abs() > 3]  #values > 3 or < -3\n",
    "\n",
    "data[(data.abs() > 3).any(axis=\"columns\")] #all rows with outliers\n",
    "data[data.abs() > 3] = np.sign(data) * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PERMUTATION AND SAMPLING**\n",
    "- `permutation`-> Reordering of rows or columns in dataframe/series in a random order\n",
    "- `sampling`-> Random selection of rows and columns from series/dataframe .It is of two types with or without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(5*7).reshape((5,7)))\n",
    "sampler_for_rows = np.random.permutation(5)\n",
    "sampler_for_columns = np.random.permutation(7)\n",
    "df.take(sampler_for_rows) #default axis rows\n",
    "df.take(sampler_for_columns , axis=\"columns\")\n",
    "\n",
    "df.sample(n=4 , axis=\"columns\" ,replace=True) #default axis is rows , default replace is False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DUMMY VARIABLES**\n",
    "- Basic Dummies: Categorical data (like \"a\", \"b\", \"c\") converting to 0 and 1\n",
    "- Prefix: to understand the new columns for less confusion\n",
    "- Multiple Categories: when a row has more than one data(like \"Action|Comedy\"), for this use str.get_dummies\n",
    "- dummies from numbers: By categories numbers into range , then make their dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\"data1\": range(6)})\n",
    "dummy_variable = pd.get_dummies(df[\"key\"],prefix=\"key\") #prefix is an optional argument\n",
    "df_with_dummy = df.join(dummy_variable) #adding it with the original dataframe\n",
    "\n",
    "mnames = [\"movie_id\", \"title\", \"genres\"]\n",
    "movies = pd.read_table(\"E:/ME/WES MECK - DATA ANALYSIS/GIT CLONE/pydata-book/datasets/movielens/movies.dat\", sep=\"::\",header=None, names=mnames, engine=\"python\")\n",
    "dummy_variable = movies[\"genres\"].str.get_dummies(\"|\") #making the while making sure to divide by the separator\n",
    "movies_dummy = movies.join(dummy_variable.add_prefix(\"genre_\"))\n",
    "\n",
    "np.random.seed(12345)\n",
    "values = np.random.uniform(size=10)\n",
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "dummy = pd.get_dummies(pd.cut(values , bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENSION DATA TYPES**\n",
    "- Since python is built on numpy , when we create null values it sees it as np.nan which might create compatability issues because it can affects the data type of our data that is being stored\n",
    "- <NA> is special value equal to pd.NA:\n",
    "- dtype=\"Int64\" is the shorthand way of writing Int data type extension\n",
    "- Importance ->\n",
    "    - Accuracy: deal correctly with missing values without changing the data type\n",
    "    - Speed: works fast on bigger data sets\n",
    "    - Memory:uses less memory ,specially for strings\n",
    "    - Flexibility: Supports special data types linke time zones etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Extension Type**    | **Description**                              |\n",
    "|-----------------------|----------------------------------------------|\n",
    "| `BooleanDtype`        | Nullable Boolean data, use `\"boolean\"` when passing as string |\n",
    "| `CategoricalDtype`    | Categorical data type, use `\"category\"` when passing as string |\n",
    "| `DatetimeTZDtype`     | Datetime with time zone                     |\n",
    "| `Float32Dtype`        | 32-bit nullable floating point, use `\"Float32\"` when passing as string |\n",
    "| `Float64Dtype`        | 64-bit nullable floating point, use `\"Float64\"` when passing as string |\n",
    "| `Int8Dtype`           | 8-bit nullable signed integer, use `\"Int8\"` when passing as string |\n",
    "| `Int16Dtype`          | 16-bit nullable signed integer, use `\"Int16\"` when passing as string |\n",
    "| `Int32Dtype`          | 32-bit nullable signed integer, use `\"Int32\"` when passing as string |\n",
    "| `Int64Dtype`          | 64-bit nullable signed integer, use `\"Int64\"` when passing as string |\n",
    "| `UInt8Dtype`          | 8-bit nullable unsigned integer, use `\"UInt8\"` when passing as string |\n",
    "| `UInt16Dtype`         | 16-bit nullable unsigned integer, use `\"UInt16\"` when passing as string |\n",
    "| `UInt32Dtype`         | 32-bit nullable unsigned integer, use `\"UInt32\"` when passing as string |\n",
    "| `UInt64Dtype`         | 64-bit nullable unsigned integer, use `\"UInt64\"` when passing as string |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1,2,3,None]) #because it has missing value pandas makes it a float dtype where as it should be a integer data type\n",
    "s = pd.Series([1,2,3,None] , dtype= pd.Int64Dtype()) #this is new Int64 with capital I data type\n",
    "s = pd.Series([\"hello\" , \"world\" , None]) #Now pandas makes it a 'object' data type\n",
    "s = pd.Series([\"hello\" , \"world\" , None],dtype = pd.StringDtype()) #This makes it a string data type\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"A\": [1, 2, None, 4],\n",
    "    \"B\": [\"one\", \"two\", \"three\", None],\n",
    "    \"C\": [False, None, False, True]\n",
    "})\n",
    "df[\"A\"] = df[\"A\"].astype(\"Int64\")\n",
    "df[\"B\"] = df[\"B\"].astype(\"string\")\n",
    "df[\"C\"] = df[\"C\"].astype(\"boolean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRING MANUPILATION**\n",
    "- Python built in string manupilation functions\n",
    "-----------\n",
    "| **Method**       | **Description**                                                                 |\n",
    "|------------------|---------------------------------------------------------------------------------|\n",
    "| `count`          | String mein substring kitni baar aaya, uski ginti return karta hai              |\n",
    "| `endswith`       | True return karta hai agar string kisi suffix se khatam hoti hai               |\n",
    "| `startswith`     | True return karta hai agar string kisi prefix se shuru hoti hai                |\n",
    "| `join`           | Ek sequence (list/tuple) ko string delimiter ke saath jodta hai                |\n",
    "| `index`          | Substring ka pehla index deta hai; nahi mila to `ValueError` raise karta hai   |\n",
    "| `find`           | Substring ka pehla index deta hai; nahi mila to `-1` return karta hai          |\n",
    "| `rfind`          | Substring ka aakhri index deta hai; nahi mila to `-1` return karta hai         |\n",
    "| `replace`        | Ek substring ko dusre se replace karta hai                                     |\n",
    "| `strip`          | Dono taraf se whitespace (spaces, newlines) hataata hai                        |\n",
    "| `rstrip`         | Right side se whitespace hataata hai                                           |\n",
    "| `lstrip`         | Left side se whitespace hataata hai                                            |\n",
    "| `split`          | String ko delimiter ke basis pe list mein todta hai                            |\n",
    "| `lower`          | Sab characters ko lowercase mein badalta hai                                   |\n",
    "| `upper`          | Sab characters ko uppercase mein badalta hai                                   |\n",
    "| `casefold`       | Lowercase mein badalta hai, aur region-specific characters ko bhi handle karta hai |\n",
    "| `ljust`          | String ko left justify karta hai, right side mein spaces add karta hai         |\n",
    "| `rjust`          | String ko right justify karta hai, left side mein spaces add karta hai         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = \"a,b,  guide\"\n",
    "\n",
    "val.split(\",\") #returns a list\n",
    "pieces = [x for x in val.strip()]\n",
    "\"::\".join(pieces) #faster then the arithmatic method\n",
    "val.index(\",\") #returns the first index of the given argument\n",
    "val.find(\",\") #does the same work like index but it does not throw an error but -1\n",
    "val.replace(\",\",\":\") #replaces the \",\" with \":\"\n",
    "val.count(\",\") #counts how many times \",\" comes in the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGULAR EXPRESSIONS**\n",
    "- Regex finds the pattern in the text and then on basis of that we can perfom multiple operations\n",
    "- Three of its major works are -> pattern matching , substitution , splitting\n",
    "- pattern expalanation \n",
    "    - [A-Z0-9._%+-]+: Username (letters, numbers, special chars).\n",
    "    - @: @ symbol.\n",
    "    - [A-Z0-9.-]+: Domain name.\n",
    "    - \\.[A-Z]{2,4}: Dot aur 2-4 letter suffix (com, net, etc.).\n",
    "\n",
    "- working with groups just put parenthesis where you have defined the patter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Method**  | **Description**                                                                                   |\n",
    "|-------------|---------------------------------------------------------------------------------------------------|\n",
    "| `findall`   | String mein saare non-overlapping matches ko list mein return karta hai                          |\n",
    "| `finditer`  | `findall` jaisa, lekin iterator return karta hai                                                 |\n",
    "| `match`     | String ke shuru mein pattern match karta hai; groups bhi de sakta hai; nahi mila to `None`       |\n",
    "| `search`    | String mein kahin bhi pehla match dhundta hai, match object deta hai                             |\n",
    "| `split`     | String ko pattern ke basis pe pieces mein todta hai                                              |\n",
    "| `sub`       | Pattern ko replacement string se replace karta hai; groups (`\\1`, `\\2`) use kar sakte hain      |\n",
    "| `subn`      | `sub` jaisa, lekin kitne replacements hue, yeh bhi batata hai (tuple mein)                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"foo    bar\\t baz  \\tqux\"\n",
    "re.split(r\"\\s+\", text) #we told it the pattern and it splits text based on that\n",
    "regex = re.compile(r\"\\s+\") #made a regex object to process it faster for reuse\n",
    "\n",
    "text = \"\"\"Dave dave@google.com\n",
    "Steve steve@gmail.com\n",
    "Rob rob@gmail.com\n",
    "Ryan ryan@yahoo.com\"\"\"\n",
    "pattern = r\"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}\"\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "regex.findall(text) #finds all matching patterns and returns a list of them\n",
    "m = regex.search(text) #gives the first pattern match in the text\n",
    "text[m.start():m.end()] #start() and end() give the index positions of the match\n",
    "regex.match(text) #None -> because it only matches at the start of the string\n",
    "regex.sub(\"reducted\", text) #substitutes 'reducted' wherever the pattern is found\n",
    "\n",
    "pattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "regex.findall(text) #list of tuples with all parts of the matched patterns\n",
    "m = regex.search(text) #finds first match, not None, use groups() for parts\n",
    "m.groups() #this returns a tuple of the matched parts, not a single string\n",
    "print(regex.match(text)) #prints None because pattern doesn't match at start\n",
    "regex.sub(r\"Username: \\1, Domain: \\2, Suffix: \\3\", text) #\\1 \\2 \\3 are the pattern groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRING FUNCTIONS IN PANDAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method      | Description  |\n",
    "|------------|-------------|\n",
    "| **cat**        | Concatenate strings element-wise with optional delimiter |\n",
    "| **contains**   | Return Boolean array if each string contains pattern/regex |\n",
    "| **count**      | Count occurrences of pattern |\n",
    "| **extract**    | Use a regular expression with groups to extract one or more strings from a Series of strings; the result will be a DataFrame with one column per group |\n",
    "| **endswith**   | Equivalent to `x.endswith(pattern)` for each element |\n",
    "| **startswith** | Equivalent to `x.startswith(pattern)` for each element |\n",
    "| **findall**    | Compute list of all occurrences of pattern/regex for each string |\n",
    "| **get**        | Index into each element (retrieve i-th element) |\n",
    "| **isalnum**    | Equivalent to built-in `str.isalnum` |\n",
    "| **isalpha**    | Equivalent to built-in `str.isalpha` |\n",
    "| **isdecimal**  | Equivalent to built-in `str.isdecimal` |\n",
    "| **isdigit**    | Equivalent to built-in `str.isdigit` |\n",
    "| **islower**    | Equivalent to built-in `str.islower` |\n",
    "| **isnumeric**  | Equivalent to built-in `str.isnumeric` |\n",
    "| **isupper**    | Equivalent to built-in `str.isupper` |\n",
    "| **join**       | Join strings in each element of the Series with passed separator |\n",
    "| **len**        | Compute length of each string |\n",
    "| **lower, upper** | Convert cases; equivalent to `x.lower()` or `x.upper()` for each element |\n",
    "| **match**      | Use `re.match` with the passed regular expression on each element, returning `True` or `False` whether it matches |\n",
    "| **pad**        | Add whitespace to left, right, or both sides of strings |\n",
    "| **center**     | Equivalent to `pad(side=\"both\")` |\n",
    "| **repeat**     | Duplicate values (e.g., `s.str.repeat(3)` is equivalent to `x * 3` for each string) |\n",
    "| **replace**    | Replace occurrences of pattern/regex with some other string |\n",
    "| **slice**      | Slice each string in the Series |\n",
    "| **split**      | Split strings on delimiter or regular expression |\n",
    "| **strip**      | Trim whitespace from both sides, including newlines |\n",
    "| **rstrip**     | Trim whitespace on right side |\n",
    "| **lstrip**     | Trim whitespace on left side |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series({\"Dave\": \"dave@google.com\",\"Steve\": \"steve@gmail.com\",\"Rob\": \"rob@gmail.com\",\"Wes\": np.nan})\n",
    "data.str.contains(\"gmails\") #to check if it contains the following\n",
    "data_string = data.astype(\"string\") #To convert from object dtype to string dtype\n",
    "\n",
    "pattern = r\"([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})\"\n",
    "import re\n",
    "data.str.findall(pattern , flags=re.IGNORECASE)\n",
    "\n",
    "match = data.str.findall(pattern , flags=re.IGNORECASE).str[0] #to get the first match of the pattern(a tuple of parts)\n",
    "match.str.get(1)  # to get the part at index 1 in the tuple\n",
    "\n",
    "data.str.extract(pattern , flags=re.IGNORECASE) #It makes the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CATEGORICAL DATA - BACKGROUNG AND MOTIVATION**\n",
    "- When in a columns there are repeated string we can assign them some code(numbers) which are gonna take less space because string themselves take too much space\n",
    "    - this is called categorical or dictionary encoded representation\n",
    "- Categorical extension types in pandas\n",
    "-------\n",
    "you can do it with any immutable objects not just with strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.Series([\"apple\",\"orange\",\"apple\",\"apple\"]*2) #column with repeated strings\n",
    "values.unique() #[\"apple\",\"orange\"]\n",
    "\n",
    "dim = pd.Series(values.unique())\n",
    "values = pd.Series([1,0,1,1]*2) #making codes to represent each categories\n",
    "dim.take(values) #This helps in saving the space in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['apple', 'orange', 'apple', 'apple'] * 2\n",
    "rng = np.random.default_rng(seed=12345)\n",
    "df = pd.DataFrame({\n",
    "    'fruit': fruits,\n",
    "    'basket_id': np.arange(len(fruits)),\n",
    "    'count': rng.integers(3, 15, size=len(fruits)),\n",
    "    'weight': rng.uniform(0, 4, size=len(fruits))\n",
    "})\n",
    "\n",
    "fruits_cat = df[\"fruit\"].astype(\"category\") #We have converted the copy of that \n",
    "#column into categorical data which takes less space in memory,this is not a string anymore \n",
    "c = fruits_cat.array #checking whether is is categorical data type\n",
    "type(c) #pandas.core.arrays.categorical.Categorical\n",
    "\n",
    "c.categories\n",
    "c.codes\n",
    "dict(enumerate(c.categories)) #to map the codes and cotegories for understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"JUST LIKE ABOVE THERE ARE MORE WAYS TO MAKE CATEGORICAL DATA TYPES\"\"\"\n",
    "#WITH SEQUENCE\n",
    "My_categorical = pd.Categorical(['foo', 'bar', 'baz', 'foo', 'bar'])\n",
    "\n",
    "#WITH CODES\n",
    "categories = ['foo', 'bar', 'baz']\n",
    "codes = [0, 1, 2, 0, 0, 1]\n",
    "my_cat = pd.Categorical.from_codes(codes , categories)\n",
    "\n",
    "#ORDERED -> like 'foo' < 'bar' < 'baz'\n",
    "my_cat.as_ordered() #method 1\n",
    "my_cat = pd.Categorical.from_codes(codes , categories , ordered=True) #method 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
